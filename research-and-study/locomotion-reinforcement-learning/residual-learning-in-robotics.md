## ✅ Residual Learning:
"걷는 동작은 어차피 서 있는 자세에서 $\pm 30^\circ$ 이상 벗어나지 않아"라는 **사전 지식(Inductive Bias)**을 주입하는 것입니다.
AI는 좁은 범위만 탐색하면 되므로, 수렴 속도가 비교도 안 되게 빠릅니다.
3. 🛡️ 안전성과 하드웨어 보호
이건 시뮬레이션보다 **Sim-to-Real(실전)**에서 더 중요합니다.

위험성: Absolute 방식으로 학습된 정책이 노이즈 때문에 튀는 값($+180^\circ$)을 뱉으면, 실제 로봇은 자기 다리로 자기 배를 걷어차거나 기어 박스를 부숴버릴 수 있습니다.
안전장치: Residual 방식은 변화량($\Delta q$)을 보통 작은 값(예: 0.5 rad)으로 클리핑(Clipping)합니다. AI가 아무리 미쳐 날뛰어도 물리적으로 로봇이 파괴될 만큼 기괴한 자세가 나오지 않도록 원천 봉쇄합니다.


## Clipping Method
네 질문의 핵심인 **"Residual Learning에서도 joint 각도가 너무 크게 변하지 않게 clipping하는 거 맞아?"**에 대한 대답은 **"YES"**입니다.

보통 두 단계로 일어납니다:

Action Clip: PPO가 뱉는 액션 값 자체를 [-1, 1]로 자릅니다.
Physical Clip: 계산된 최종 $q_{target}$이 로봇의 관절 한계(Joint Limit, 예: 무릎은 -180도까지만 펴짐)를 넘으면 거기서 또 자릅니다.




 filtered_action = alpha * action + (1 - alpha) * self.last_action



-> 지수 이동 평균(Exponential Moving Average, EMA) 또는 **1차 저역 통과 필터(First-order Low-Pass Filter)**다.



물리적 효과: 로봇의 반응 속도를 강제로 늦춘다.

만약 AI가 갑자기 "오른쪽으로 확 꺾어! (+1.0)"라고 해도, 실제로는 $0.5 \times 1.0 + 0.5 \times 0.0 = 0.5$만큼만 움직인다.
다음 스텝이 되어서야 비로소 더 많이 꺾는다.

왜 쓰는가? (Sim-to-Real)

현실의 모터는 즉각 반응하지 않는다. 전기를 준다고 0초 만에 모터가 돌지 않는다(관성, 인덕턴스 때문).
시뮬레이션에서 이 필터를 쓰지 않으면 로봇은 빛의 속도로 반응하는 "슈퍼 로봇"이 되어버리고, 현실에 가져오면 "어? 왜 이렇게 굼뜨지?" 하며 넘어진다.
즉, 일부러 현실 모터의 굼뜬 반응(Delay/Lag)을 모사하기 위해 넣은 코드다.
