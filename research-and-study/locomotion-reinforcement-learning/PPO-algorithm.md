$$D_{KL}(\pi_{old} || \pi_{new}) = \mathbb{E} \left[ \log \frac{\pi_{old}(a|s)}{\pi_{new}(a|s)} \right] = \mathbb{E} [ \log \pi_{old}(a|s) - \log \pi_{new}(a|s) ]$$
-> approx_kl(KL Divergence(쿨백-라이블러 발산)는 두 확률 분포($P$와 $Q$)가 얼마나 다른지를 측정하는 통계학적 거리입니다.)이 의미하는 것 : 얼만큼의 업데이트(policy의 생각이 바뀐 크기)가 이루어졌는지 수학적으로 표현. -> 0.02값이 적당. 


- log의 이유 : 미분을 편리하게 하기 위해서 왜냐하면 곱셈 -> 덧셈으로 해주기 때문에
- 스케일링. 

## 실제로 업데이트 시에 사용하는 함수(손실함수)
$$r_t(\theta) = \frac{\pi_{new}(a|s)}{\pi_{old}(a|s)}$$

- PPO의 핵심 (Clipping):만약 $R$이 너무 커져서(예: 갑자기 100배), $1.2$ (또는 $1-\epsilon, 1+\epsilon$)를 넘어가면 "야, 너무 많이 변했어! 잘라(Clip)!" 라고 강제로 업데이트를 막아버립니다.이게 바로 PPO의 Clipped Surrogate Objective입니다.


2. 📏 approx_kl : 속도계 (모니터링용)반면, 로그에 찍히는 approx_kl은 이 업데이트가 끝난 뒤 "그래서 결과적으로 분포가 얼마나 달라졌는데?" 를 재는 **척도(Metric)**입니다.역할: 확률값 하나하나의 비율($R$)이 아니라, 확률 분포 전체(뇌의 상태)가 수학적으로 얼마나 멀어졌는지를 측정합니다.왜 $R$ 대신 KL을 로그로 볼까?$R$은 행동 하나하나마다 값이 다 달라서(어떤 건 1.1배, 어떤 건 0.9배), 전체적인 학습 안정성을 숫자로 한눈에 보기 힘듭니다.$KL$은 정보이론적으로 **"두 분포의 차이"**를 하나의 숫자로 깔끔하게 요약해줍니다. 그래서 "학습이 안정적인가?"를 판단할 때 엔지니어들이 주로 보는 지표가 된 것입니다.


3. 📏 정보이론의 정의 (KL Divergence의 본질)마지막으로 approx_kl에서 로그를 쓰는 근본적인 이유는, **정보량(Information)**의 정의 자체가 로그이기 때문입니다.정보량의 정의: $I(x) = -\log P(x)$의미: "희귀한 사건일수록(확률 $P$가 낮을수록) 정보량이 많다."KL Divergence는 통계적인 거리이기도 하지만, 본질적으로는 **"내 예전 생각($\pi_{old}$)을 새로운 생각($\pi_{new}$)으로 대체할 때 발생하는 '정보량의 손실(엔트로피 차이)'"**을 재는 것입니다.정보량을 재야 하니 당연히 공식에 로그($\log$)가 들어갈 수밖에 없습니다.