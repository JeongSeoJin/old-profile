# 학습의 1 사이클 (순서대로)
## 문제 풀기 (Forward Propagation):

모델이 입력 데이터(손동작)를 보고 "이건 70% 확률로 'Come'이야"라고 예측합니다.

## 채점 하기 (Loss Calculation):

실제 정답('Come')과 비교해보니 30%만큼 틀렸네? (오차 계산)

## 범인 찾기 (Backpropagation, 역전파):

"누구 때문에 틀렸어?"라며 뒤로 거슬러 올라가서, 각 가중치(파라미터)들의 책임(기울기, Gradient)을 계산합니다.

## ★ 최적화 알고리즘 작용 (Weight Update):

바로 이때 등장!

"너는 값을 좀 줄이고, 너는 값을 좀 키워."

계산된 책임을 바탕으로 **실제 가중치 숫자를 갱신(Update)**합니다.

즉, train.ipynb를 실행해서 학습이 진행되는 동안, 수천 번, 수만 번 반복해서 "크리크 수정"을 수행합니다.